{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THINGSvision\n",
    "This is the PyTorch version, you can find a TensorFlow example [here](https://colab.research.google.com/github/ViCCo-Group/THINGSvision/blob/master/doc/tensorflow.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ICWd-3iA671"
   },
   "source": [
    "### Install thingsvision and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h0nVMt-M_KX_"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade thingsvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yq-bNySyBGO-"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from thingsvision import get_extractor\n",
    "from thingsvision.utils.storing import save_features\n",
    "from thingsvision.utils.data import ImageDataset, DataLoader\n",
    "from thingsvision.core.extraction import center_features\n",
    "\n",
    "from google.colab import drive\n",
    "from typing import Any, Dict, List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image and feature directories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PHSuNkaIAZw2"
   },
   "source": [
    "Specify both `path/to/images` (input directory) and `path/to/features` (output directory) on your Google Drive. \n",
    "The image directory is expected to contain images that are saved similarly to `/dog/img_1.png` or `/cat/img_1.jpg`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = 'path/to/images'  # path/to/images in GDrive\n",
    "output_dir = 'path/to/features' # path/to/output  in GDrive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xF0R7sFu-7gI"
   },
   "source": [
    "Mount Google Drive "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I8nY_u1p-1F6",
    "outputId": "fd0e4a58-872c-4bfd-fb79-17586d6e48ce"
   },
   "outputs": [],
   "source": [
    "mounted_dir = '/thingsvision'\n",
    "drive.mount(mounted_dir, force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_image_path = os.path.join(mounted_dir, 'MyDrive', image_dir)\n",
    "full_output_path = os.path.join(mounted_dir, 'MyDrive', output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions to extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 383
    },
    "id": "MkYIhI_P_Z6t",
    "outputId": "d1d1f8d3-16bc-43bf-f4ff-7b4a292e274f"
   },
   "outputs": [],
   "source": [
    "def extract_features(\n",
    "                    extractor: Any,\n",
    "                    module_name: str,\n",
    "                    image_path: str,\n",
    "                    out_path: str,\n",
    "                    batch_size: int,\n",
    "                    flatten_activations: bool,\n",
    "                    apply_center_crop: bool,\n",
    "                    class_names: List[str]=None,\n",
    "                    file_names: List[str]=None,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Extract features for a single layer.\"\"\"                                    \n",
    "    dataset = ImageDataset(\n",
    "        root=image_path,\n",
    "        out_path=out_path,\n",
    "        backend=extractor.get_backend(),\n",
    "        transforms=extractor.get_transformations(apply_center_crop=apply_center_crop),\n",
    "        class_names=class_names,\n",
    "        file_names=file_names,\n",
    "    )\n",
    "    batches = DataLoader(dataset=dataset, batch_size=batch_size, backend=extractor.get_backend())\n",
    "    features = extractor.extract_features(\n",
    "                    batches=batches,\n",
    "                    module_name=module_name,\n",
    "                    flatten_acts=flatten_activations,\n",
    "    )\n",
    "    return features\n",
    "\n",
    "\n",
    "def extract_all_layers(\n",
    "                        model_name: str,\n",
    "                        extractor: Any,\n",
    "                        image_path: str,\n",
    "                        out_path: str,\n",
    "                        batch_size: int,\n",
    "                        flatten_activations: bool,\n",
    "                        apply_center_crop: bool,\n",
    "                        layer: Any=nn.Linear,\n",
    "                        class_names: List[str]=None,\n",
    "                        file_names: List[str]=None,\n",
    ") -> Dict[str, np.ndarray]:\n",
    "    \"\"\"Extract features for all selected layers and save them to disk.\"\"\"\n",
    "    features_per_layer = {}\n",
    "    for l, (module_name, module) in enumerate(extractor.model.named_modules(), start=1):\n",
    "        if isinstance(module, layer):\n",
    "            # extract features for layer \"module_name\"\n",
    "            features = extract_features(\n",
    "                                        extractor=extractor,\n",
    "                                        module_name=module_name,\n",
    "                                        image_path=image_path,\n",
    "                                        out_path=out_path,\n",
    "                                        batch_size=batch_size,\n",
    "                                        flatten_activations=flatten_activations,\n",
    "                                        apply_center_crop=apply_center_crop,\n",
    "                                        class_names=class_names,\n",
    "                                        file_names=file_names,\n",
    "            )\n",
    "            # replace with e.g., [f'conv_{l:02d}'] or [f'fc_{l:02d}']\n",
    "            features_per_layer[f'layer_{l:02d}'] = features\n",
    "            # save features to disk\n",
    "            save_features(features, out_path=f'{out_path}/features_{model_name}_{module_name}', file_format='npy')\n",
    "    return features_per_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fvJFUBiJ_j0W"
   },
   "outputs": [],
   "source": [
    "pretrained = True # use pretrained model weights\n",
    "model_path = None # if pretrained = False (i.e., randomly initialized weights) set path to model weights\n",
    "batch_size = 32 # use a power of two (this can be any size, depending on the number of images for which you aim to extract features)\n",
    "apply_center_crop = True # center crop images (set to False, if you don't want to center-crop images)\n",
    "flatten_activations = True # whether or not features (e.g., of Conv layers) should be flattened\n",
    "class_names = None  # optional list of class names for class dataset\n",
    "file_names = None # optional list of file names according to which features should be sorted\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select `model` and `layer` for which you want to extract image features. If you want to extract features from a `torchvision` model, use the model naming defined [here](https://pytorch.org/vision/stable/models.html) (e.g., `vgg16` if you want to use VGG-16). If you are uncertain about the naming and enumeration of the layers, use `model.show()` to see how specific layers called."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: VGG-16 with batch norm (pretrained on ImageNet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that it is crucial to set a model's `source`. VGG16 implementations exist in different libraries and therefore (pretrained) weights can be downloaded from different sources. One such source is `torchvision` from which we will download VGG16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model_name = 'vgg16_bn' \n",
    "# specify model source \n",
    "# we use torchvision here (https://pytorch.org/vision/stable/models.html)\n",
    "source = 'torchvision'\n",
    "# initialize the extractor\n",
    "extractor = get_extractor( \n",
    "            model_name=model_name,\n",
    "            pretrained=pretrained,\n",
    "            model_path=model_path,\n",
    "            device=device,\n",
    "            source=source\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## select layer\n",
    "\n",
    "# NOTE: uncomment the line below, if you are uncertain about layer naming in PyTorch\n",
    "# module_name = model.show() \n",
    "module_name = 'features.23' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature extraction single layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# extract features for a single layer (e.g., penultimate)\n",
    "features = extract_features(\n",
    "                            extractor=extractor,\n",
    "                            module_name=module_name,\n",
    "                            image_path=full_image_path,\n",
    "                            out_path=full_output_path,\n",
    "                            batch_size=batch_size,\n",
    "                            flatten_activations=flatten_activations,\n",
    "                            apply_center_crop=apply_center_crop,\n",
    "                            class_names=class_names,\n",
    "                            file_names=file_names,\n",
    ")\n",
    "\n",
    "# apply centering (not necessary, but may be desirable, depending on the analysis)\n",
    "features = center_features(features)\n",
    "\n",
    "# save features to disk\n",
    "save_features(features, out_path=f'{full_output_path}/features_{model_name}_{module_name}', file_format='npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature extraction all convolutional or fully-connected layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features for all convolutional layers (i.e., Conv2d) and save them to disk\n",
    "layer = nn.Conv2d\n",
    "features_conv_layers = extract_all_layers(\n",
    "                                            model_name=model_name,\n",
    "                                            extractor=extractor,\n",
    "                                            image_path=full_image_path,\n",
    "                                            out_path=full_output_path,\n",
    "                                            batch_size=batch_size,\n",
    "                                            flatten_activations=flatten_activations,\n",
    "                                            apply_center_crop=apply_center_crop,\n",
    "                                            layer=layer,\n",
    "                                            class_names=class_names,\n",
    "                                            file_names=file_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features for all fully-connected layers (i.e., Linear) and save them to disk\n",
    "layer = nn.Linear\n",
    "features_fc_layers = extract_all_layers(\n",
    "                                        model_name=model_name,\n",
    "                                        extractor=extractor,\n",
    "                                        image_path=full_image_path,\n",
    "                                        out_path=full_output_path,\n",
    "                                        batch_size=batch_size,\n",
    "                                        flatten_activations=flatten_activations,\n",
    "                                        apply_center_crop=apply_center_crop,\n",
    "                                        layer=layer,\n",
    "                                        class_names=class_names,\n",
    "                                        file_names=file_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: VGG-16 (pretrained on Ecoset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## load model\n",
    "model_name = 'VGG16_ecoset'\n",
    "# specifiy model source\n",
    "# the model's source here is custom\n",
    "source = 'custom'\n",
    "extractor = get_extractor(\n",
    "            model_name=model_name,\n",
    "            pretrained=pretrained,\n",
    "            model_path=model_path,\n",
    "            device=device,\n",
    "            source=source,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## select layer\n",
    "\n",
    "# NOTE: uncomment the line below, if you are uncertain about layer naming in PyTorch\n",
    "# module_name = model.show() \n",
    "module_name = 'features.23' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature extraction single layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features for a single layer (e.g., penultimate)\n",
    "vgg_ecoset_features = extract_features(\n",
    "                            extractor=extractor,\n",
    "                            module_name=module_name,\n",
    "                            image_path=full_image_path,\n",
    "                            out_path=full_output_path,\n",
    "                            batch_size=batch_size,\n",
    "                            flatten_activations=flatten_activations,\n",
    "                            apply_center_crop=apply_center_crop,\n",
    "                            class_names=class_names,\n",
    "                            file_names=file_names,\n",
    ")\n",
    "\n",
    "# apply centering (not necessary, but may be desirable, depending on the analysis)\n",
    "vgg_ecoset_features = center_features(vgg_ecoset_features)\n",
    "\n",
    "# save features to disk\n",
    "save_features(vgg_ecoset_features, out_path=f'{full_output_path}/features_{model_name}_{module_name}', file_format='npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature extraction all convolutional or fully-connected layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features for all convolutional layers (i.e., Conv2d) and save them to disk\n",
    "layer = nn.Conv2d\n",
    "features_conv_layers = extract_all_layers(\n",
    "                                            model_name=model_name,\n",
    "                                            extractor=extractor,\n",
    "                                            image_path=full_image_path,\n",
    "                                            out_path=full_output_path,\n",
    "                                            batch_size=batch_size,\n",
    "                                            flatten_activations=flatten_activations,\n",
    "                                            apply_center_crop=apply_center_crop,\n",
    "                                            layer=layer,\n",
    "                                            class_names=class_names,\n",
    "                                            file_names=file_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features for all fully-connected layers (i.e., Linear) and save them to disk\n",
    "layer = nn.Linear\n",
    "features_fc_layers = extract_all_layers(\n",
    "                                        model_name=model_name,\n",
    "                                        extractor=extractor,\n",
    "                                        image_path=full_image_path,\n",
    "                                        out_path=full_output_path,\n",
    "                                        batch_size=batch_size,\n",
    "                                        flatten_activations=flatten_activations,\n",
    "                                        apply_center_crop=apply_center_crop,\n",
    "                                        layer=layer,\n",
    "                                        class_names=class_names,\n",
    "                                        file_names=file_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: CLIP (multimodal pretraining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model_name = 'clip'\n",
    "variant = 'ViT-B/32'\n",
    "# specifiy model source\n",
    "# the model's source here is custom\n",
    "source = 'custom'\n",
    "extractor = get_extractor( \n",
    "            model_name=model_name,\n",
    "            pretrained=pretrained,\n",
    "            model_path=model_path,\n",
    "            device=device,\n",
    "            source=source,\n",
    "            model_parameters={'variant': variant}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## select layer\n",
    "\n",
    "# NOTE: uncomment the line below, if you are uncertain about layer naming of CLIP\n",
    "# module_name = model.show()\n",
    "module_name = 'visual' # penultimate layer in CLIP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature extraction single layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features\n",
    "clip_features = extract_features(\n",
    "                            extractor=extractor,\n",
    "                            module_name=module_name,\n",
    "                            image_path=full_image_path,\n",
    "                            out_path=full_output_path,\n",
    "                            batch_size=batch_size,\n",
    "                            flatten_activations=flatten_activations,\n",
    "                            apply_center_crop=apply_center_crop,\n",
    "                            class_names=class_names,\n",
    "                            file_names=file_names,\n",
    ")\n",
    "\n",
    "# apply centering (not necessary, but may be desirable, depending on the analysis)\n",
    "clip_features = center_features(clip_features)\n",
    "\n",
    "# save features to disk\n",
    "save_features(clip_features, out_path=f'{full_output_path}/features_{model_name}_{module_name}', file_format='npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representational Similarity Analysis (RSA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thingsvision.core.rsa import compute_rdm, plot_rdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute representational dissimilarity matrix (RDM) for CLIP features\n",
    "rdm = compute_rdm(clip_features, method='correlation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot rdm\n",
    "plot_rdm(\n",
    "        full_output_path,\n",
    "        clip_features,\n",
    "        method='correlation',\n",
    "        format='.png', # '.jpg'\n",
    "        colormap='cividis',\n",
    "        show_plot=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Centered Kernel Alignment (CKA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thingsvision.core.cka import CKA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert vgg_ecoset_features.shape[0] == clip_features.shape[0]\n",
    "m = clip_features.shape[0]\n",
    "cka = CKA(m=m, kernel='linear')\n",
    "rho = cka.compare(X=vgg_ecoset_features, Y=clip_features)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "THINGSVISION.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
