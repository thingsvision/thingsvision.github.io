{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THINGSvision\n",
    "This is the TensorFlow version, you can find a PyTorch example [here](https://colab.research.google.com/github/ViCCo-Group/THINGSvision/blob/master/doc/pytorch.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ICWd-3iA671"
   },
   "source": [
    "### Install thingsvision and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h0nVMt-M_KX_"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade thingsvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yq-bNySyBGO-"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import tensorflow\n",
    "import re\n",
    "import numpy as np\n",
    "from thingsvision import get_extractor\n",
    "from thingsvision.utils.storing import save_features\n",
    "from thingsvision.utils.data import ImageDataset, DataLoader\n",
    "\n",
    "from google.colab import drive\n",
    "from typing import Any, Dict, List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image and feature directories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PHSuNkaIAZw2"
   },
   "source": [
    "Specify both `path/to/images` (input directory) and `path/to/features` (output directory) on your Google Drive. \n",
    "The image directory is expected to contain images that are saved similarly to `/dog/img_1.png` or `/cat/img_1.jpg`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = 'path/to/images'  # path/to/images in GDrive\n",
    "output_dir = 'path/to/features' # path/to/output  in GDrive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xF0R7sFu-7gI"
   },
   "source": [
    "Mount Google Drive "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I8nY_u1p-1F6",
    "outputId": "fd0e4a58-872c-4bfd-fb79-17586d6e48ce"
   },
   "outputs": [],
   "source": [
    "mounted_dir = '/thingsvision'\n",
    "drive.mount(mounted_dir, force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_image_path = os.path.join(mounted_dir, 'MyDrive', image_dir)\n",
    "full_output_path = os.path.join(mounted_dir, 'MyDrive', output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions to extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 383
    },
    "id": "MkYIhI_P_Z6t",
    "outputId": "d1d1f8d3-16bc-43bf-f4ff-7b4a292e274f"
   },
   "outputs": [],
   "source": [
    "def extract_features(\n",
    "                    extractor: Any,\n",
    "                    module_name: str,\n",
    "                    image_path: str,\n",
    "                    out_path: str,\n",
    "                    batch_size: int,\n",
    "                    flatten_activations: bool,\n",
    "                    apply_center_crop: bool,\n",
    "                    class_names: List[str]=None,\n",
    "                    file_names: List[str]=None,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Extract features for a single layer.\"\"\"                                    \n",
    "    dataset = ImageDataset(\n",
    "        root=image_path,\n",
    "        out_path=out_path,\n",
    "        backend=extractor.get_backend(),\n",
    "        transforms=extractor.get_transformations(apply_center_crop=apply_center_crop, resize_dim=256, crop_dim=224),\n",
    "        class_names=class_names,\n",
    "        file_names=file_names,\n",
    "    )\n",
    "    batches = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size, \n",
    "        backend=extractor.get_backend(),\n",
    "        )\n",
    "    features = extractor.extract_features(\n",
    "                    batches=batches,\n",
    "                    module_name=module_name,\n",
    "                    flatten_acts=flatten_activations,\n",
    "    )\n",
    "    return features\n",
    "\n",
    "\n",
    "def get_module_names(modules: List[Any]) -> List[str]:\n",
    "    \"\"\"Yield module names associated with layers.\"\"\"\n",
    "    return list(map(lambda m: m.name, modules))\n",
    "\n",
    "\n",
    "def extract_all_layers(\n",
    "                        model_name: str,\n",
    "                        extractor: Any,\n",
    "                        image_path: str,\n",
    "                        out_path: str,\n",
    "                        batch_size: int,\n",
    "                        flatten_activations: bool,\n",
    "                        apply_center_crop: bool,\n",
    "                        layer: str='conv',\n",
    "                        class_names: List[str]=None,\n",
    "                        file_names: List[str]=None,\n",
    ") -> Dict[str, np.ndarray]:\n",
    "    \"\"\"Extract features for all selected layers and save them to disk.\"\"\"\n",
    "    features_per_layer = {}\n",
    "    module_names = get_module_names(extractor.model.layers)\n",
    "    for l, module_name in enumerate(module_names, start=1):\n",
    "        if re.search(f'{layer}', module_name):\n",
    "            # extract features for layer \"module_name\"\n",
    "            features = extract_features(\n",
    "                                        extractor=extractor,\n",
    "                                        module_name=module_name,\n",
    "                                        image_path=image_path,\n",
    "                                        out_path=out_path,\n",
    "                                        batch_size=batch_size,\n",
    "                                        flatten_activations=flatten_activations,\n",
    "                                        apply_center_crop=apply_center_crop,\n",
    "                                        class_names=class_names,\n",
    "                                        file_names=file_names,\n",
    "            )\n",
    "            # replace with e.g., [f'conv_{l:02d}'] or [f'fc_{l:02d}']\n",
    "            features_per_layer[f'{layer}_{l:02d}'] = features\n",
    "            # save features to disk\n",
    "            save_features(features, out_path=f'{out_path}/features_{model_name}_{module_name}', file_format='npy')\n",
    "    return features_per_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fvJFUBiJ_j0W"
   },
   "outputs": [],
   "source": [
    "pretrained = True # use pretrained model weights\n",
    "model_path = None # if pretrained = False (i.e., randomly initialized weights) set path to model weights\n",
    "batch_size = 32 # use a power of two (this can be any size, depending on the number of images for which you aim to extract features)\n",
    "apply_center_crop = True # center crop images (set to False, if you don't want to center-crop images)\n",
    "flatten_activations = True # whether or not features (e.g., of Conv layers) should be flattened\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "class_names = None  # optional list of class names for class dataset\n",
    "file_names = None # optional list of file names according to which features should be sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select `model` and `layer` for which you want to extract image features. If you want to extract features from a `torchvision` model, use the model naming defined [here](https://pytorch.org/vision/stable/models.html) (e.g., `vgg16` if you want to use VGG-16). If you are uncertain about the naming and enumeration of the layers, use `model.show()` to see how specific layers called."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG-16 (pretrained on ImageNet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that it is crucial to set a model's `source`. VGG16 implementations exist in different libraries and therefore (pretrained) weights can be downloaded from different sources. One such source is `keras` from which we will download VGG16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## load model\n",
    "model_name = 'VGG16' \n",
    "# specify model source \n",
    "# we use keras (backend is TensorFlow) here (https://keras.io/api/applications/)\n",
    "source = 'keras' \n",
    "\n",
    "extractor = get_extractor(\n",
    "            model_name=model_name,\n",
    "            pretrained=pretrained,\n",
    "            model_path=model_path,\n",
    "            device=device,\n",
    "            source=source\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## select layer\n",
    "\n",
    "# NOTE: uncomment the line below, if you are uncertain about layer naming in TensorFlow\n",
    "# module_name = model.show() \n",
    "module_name = 'fc1' # 'block1_conv1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature extraction single layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features for a single layer (e.g., penultimate)\n",
    "features = extract_features(\n",
    "                            extractor=extractor,\n",
    "                            module_name=module_name,\n",
    "                            image_path=full_image_path,\n",
    "                            out_path=full_output_path,\n",
    "                            batch_size=batch_size,\n",
    "                            flatten_activations=flatten_activations,\n",
    "                            apply_center_crop=apply_center_crop,\n",
    "                            class_names=class_names,\n",
    "                            file_names=file_names,\n",
    ")\n",
    "\n",
    "# save features to disk\n",
    "save_features(features, out_path=f'{full_output_path}/features_{model_name}_{module_name}', file_format='npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature extraction all convolutional or fully-connected layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features for all convolutional layers (i.e., block#_conv#) and save them to disk\n",
    "layer = 'conv'\n",
    "features_conv_layers = extract_all_layers(\n",
    "                                            extractor=extractor,\n",
    "                                            model_name=model_name,\n",
    "                                            image_path=full_image_path,\n",
    "                                            out_path=full_output_path,\n",
    "                                            batch_size=batch_size,\n",
    "                                            flatten_activations=flatten_activations,\n",
    "                                            apply_center_crop=apply_center_crop,\n",
    "                                            layer=layer,\n",
    "                                            class_names=class_names,\n",
    "                                            file_names=file_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features for all fully-connected layers (i.e., fc#) and save them to disk\n",
    "layer = 'fc'\n",
    "features_fc_layers = extract_all_layers(\n",
    "                                        extractor=extractor,\n",
    "                                        model_name=model_name,\n",
    "                                        image_path=full_image_path,\n",
    "                                        out_path=full_output_path,\n",
    "                                        batch_size=batch_size,\n",
    "                                        flatten_activations=flatten_activations,\n",
    "                                        apply_center_crop=apply_center_crop,\n",
    "                                        layer=layer,\n",
    "                                        class_names=class_names,\n",
    "                                        file_names=file_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representational Similarity Analysis (RSA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thingsvision.core.rsa import compute_rdm, plot_rdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute representational dissimilarity matrix\n",
    "rdm = compute_rdm(features, method='correlation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot rdm\n",
    "plot_rdm(\n",
    "            full_output_path,\n",
    "            features,\n",
    "            method='correlation',\n",
    "            format='.png', # '.jpg'\n",
    "            colormap='cividis',\n",
    "            show_plot=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Centered Kernel Alignment (CKA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thingsvision.core.cka import CKA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_i = features_fc_layers[f'fc_01']\n",
    "features_j = features_fc_layers[f'fc_02']\n",
    "\n",
    "assert features_i.shape[0] == features_j.shape[0]\n",
    "m = features_i.shape[0]\n",
    "cka = CKA(m=m, kernel='linear')\n",
    "rho = cka.compare(X=features_i, Y=features_j)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "THINGSVISION.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
